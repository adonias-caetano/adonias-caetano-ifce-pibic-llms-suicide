{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaqJ7eVlBv09"
      },
      "source": [
        "#  XAI BERT- Suicide Detection\n",
        "**Autora:** Lais Carvalho Coutinho\n",
        "\n",
        "**Orientador:** Adonias Caetano de Oliveira\n",
        "\n",
        "**Instituição:** IFCE\n",
        "\n",
        "**Dataset disponível em:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c3ex9p6B6a2"
      },
      "source": [
        "## Instalação de Pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3-CiHxmXE9_"
      },
      "outputs": [],
      "source": [
        "!pip install Unidecode transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhGy9D_7B9Z9"
      },
      "source": [
        "## Importação de Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irNhcn15X9Bs"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp3iYSc_X_iC"
      },
      "outputs": [],
      "source": [
        "#text preprocessing libraries\n",
        "\n",
        "import re\n",
        "import math\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_curve, auc, ConfusionMatrixDisplay, confusion_matrix\n",
        "from scipy.special import expit\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# import seaborn as sns\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from unidecode import unidecode\n",
        "# from string import punctuation\n",
        "# # from wordcloud import WordCloud\n",
        "# from lime.lime_text import LimeTextExplainer\n",
        "# from scipy.special import expit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tppRsT1ZCHbZ"
      },
      "source": [
        "## Carregamento do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_LgvDKLkMc1"
      },
      "outputs": [],
      "source": [
        "link_train = 'link púbico do Google Drive de acesso ao CSV de treinamento'\n",
        "link_test = 'link púbico do Google Drive de acesso ao CSV de teste'\n",
        "\n",
        "file_id_link_train = link_train.split('/')[-2]\n",
        "file_id_link_test = link_test.split('/')[-2]\n",
        "\n",
        "read_link_train = 'https://drive.google.com/uc?id=' + file_id_link_train\n",
        "read_link_test = 'https://drive.google.com/uc?id=' + file_id_link_test\n",
        "\n",
        "train_dataset = pd.read_csv(read_link_train,  index_col=0)\n",
        "test_dataset = pd.read_csv(read_link_test,  index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNDOsb5OgTpP"
      },
      "outputs": [],
      "source": [
        "print(test_dataset.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-hK3UNJNipS"
      },
      "outputs": [],
      "source": [
        "print(train_dataset['classification'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUhcY-XYCcbp"
      },
      "source": [
        "## Divisão dos Dados em Conjuntos de Treinamento, Validação e Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtnfmyswYQKr"
      },
      "outputs": [],
      "source": [
        "# Dividindo os dados de treino em treino e validação\n",
        "train_df, valid_df, train_label, valid_label = train_test_split(\n",
        "    train_dataset[['text']], train_dataset['classification'], test_size=0.20, random_state=42)\n",
        "\n",
        "# Dividindo os dados de teste (já definidos)\n",
        "test_df = test_dataset[['text']]\n",
        "test_label = test_dataset['classification']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeaKWi6qYV28"
      },
      "outputs": [],
      "source": [
        "print(\"Conjunto de Treinamento:\")\n",
        "print(train_df.head(), \"\\n\")\n",
        "print(\"Conjunto de Validação:\")\n",
        "print(valid_df.head(), \"\\n\")\n",
        "print(\"Conjunto de Teste:\")\n",
        "print(test_df.head(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YrjI43mhR2a"
      },
      "outputs": [],
      "source": [
        "# Garantir que todos os valores na coluna 'text' sejam strings\n",
        "train_df['text'] = train_df['text'].fillna('').astype(str)\n",
        "valid_df['text'] = valid_df['text'].fillna('').astype(str)\n",
        "test_df['text'] = test_df['text'].fillna('').astype(str)\n",
        "\n",
        "train_texts = train_df['text'].values.tolist()\n",
        "valid_texts = valid_df['text'].values.tolist()\n",
        "test_texts = test_df['text'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZbDmQPoAriS"
      },
      "outputs": [],
      "source": [
        "print(f\"Train: {len(train_df)} lines\")\n",
        "print(f\"Validation: {len(valid_df)} lines\")\n",
        "print(f\"Test: {len(test_df)} lines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd6J4GMUAZCM"
      },
      "outputs": [],
      "source": [
        "label_names = ['non-suicide', 'suicide']\n",
        "label_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeP2uHqdCjHf"
      },
      "source": [
        "## Inicialização do Tokenizador e do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ti-RJsTbtBK"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_LM = 'bert-large-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM, do_lower_case=True)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEdlzECHCnw4"
      },
      "source": [
        "## Definição de Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0_7aAiBasNn"
      },
      "outputs": [],
      "source": [
        "def encode(docs):\n",
        "    '''\n",
        "    This function takes list of texts and returns input_ids and attention_mask of texts\n",
        "    '''\n",
        "    encoded_dict = tokenizer.batch_encode_plus(docs,\n",
        "                                               add_special_tokens=True,\n",
        "                                               max_length=128,\n",
        "                                               padding='max_length',\n",
        "                                               return_attention_mask=True,\n",
        "                                               truncation=True,\n",
        "                                               return_tensors='pt')\n",
        "    input_ids = encoded_dict['input_ids']\n",
        "    attention_masks = encoded_dict['attention_mask']\n",
        "    return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch3SSX5WCsBw"
      },
      "source": [
        "## Preparação dos dados para o treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEn5vxVQa34p"
      },
      "outputs": [],
      "source": [
        "train_input_ids, train_att_masks = encode(train_df['text'].values.tolist())\n",
        "valid_input_ids, valid_att_masks = encode(valid_df['text'].values.tolist())\n",
        "test_input_ids, test_att_masks = encode(test_df['text'].values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ_95EXXcZFM"
      },
      "outputs": [],
      "source": [
        "train_y = torch.LongTensor(train_label.values)\n",
        "valid_y = torch.LongTensor(valid_label.values)\n",
        "test_y = torch.LongTensor(test_label.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkjZRX-ccih3"
      },
      "outputs": [],
      "source": [
        "N_labels = len(train_label.unique())\n",
        "model = BertForSequenceClassification.from_pretrained(PRETRAINED_LM,\n",
        "                                                      num_labels=N_labels,\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCCFQ3B1CPdc"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLVbBeOjccYI"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
        "valid_sampler = SequentialSampler(valid_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdOHFZsvcfqP"
      },
      "outputs": [],
      "source": [
        "train_label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqDyhGFDclar"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC7jWniPcmW1"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ubCJJBQcq2a"
      },
      "outputs": [],
      "source": [
        "# Best results: 07 and 08\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 2e-6\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "             num_warmup_steps=0,\n",
        "            num_training_steps=len(train_dataloader)*EPOCHS )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m94lfbg-C4f0"
      },
      "source": [
        "## Treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sk8ABQscwKO"
      },
      "outputs": [],
      "source": [
        "train_loss_per_epoch = []\n",
        "val_loss_per_epoch = []\n",
        "\n",
        "for epoch_num in range(EPOCHS):\n",
        "    print('Epoch: ', epoch_num + 1)\n",
        "    '''\n",
        "    Training\n",
        "    '''\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(tqdm(train_dataloader,desc='Training')):\n",
        "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
        "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
        "\n",
        "        loss = output.loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        del loss\n",
        "\n",
        "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    train_loss_per_epoch.append(train_loss / (step_num + 1))\n",
        "\n",
        "\n",
        "    '''\n",
        "    Validation\n",
        "    '''\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_pred = []\n",
        "    with torch.no_grad():\n",
        "        for step_num_e, batch_data in enumerate(tqdm(valid_dataloader,desc='Validation')):\n",
        "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
        "            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
        "\n",
        "            loss = output.loss\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
        "\n",
        "    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n",
        "    valid_pred = np.concatenate(valid_pred)\n",
        "\n",
        "    '''\n",
        "    Loss message\n",
        "    '''\n",
        "    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(train_df) / BATCH_SIZE), train_loss / (step_num + 1)))\n",
        "    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(valid_df) / BATCH_SIZE), valid_loss / (step_num_e + 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz7WO3Fgc0Qj"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, EPOCHS +1 )\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(epochs,train_loss_per_epoch,label ='training loss')\n",
        "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
        "ax.set_title('Training and Validation loss')\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vHwgXPC-Fi"
      },
      "source": [
        "## Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1I8IBJUUcKs"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_pred = []\n",
        "test_loss= 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
        "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
        "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
        "\n",
        "        loss = output.loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        test_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
        "test_pred = np.concatenate(test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O947EGtec5-X"
      },
      "outputs": [],
      "source": [
        "print('classification report')\n",
        "\n",
        "print(classification_report(test_pred, test_label.to_numpy(),target_names=label_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk_D_bFsDh-f"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
        "      input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
        "      output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
        "\n",
        "      prob = expit(output.logits.cpu().detach().numpy())\n",
        "\n",
        "      probabilities.append( prob )\n",
        "\n",
        "probabilities = np.concatenate(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1poHrRfDjX2"
      },
      "outputs": [],
      "source": [
        "probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbNeI3snWRPC"
      },
      "outputs": [],
      "source": [
        "print(type(test_label))\n",
        "print(type(test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPjKFuMufRSX"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(fper, tper, AUC):\n",
        "    plt.plot(fper, tper, color=\"red\", label=f\"AUC = {AUC}\")\n",
        "    plt.plot([0, 1], [0, 1], color=\"green\", linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"Receiver Operating Characteristic Curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "fper, tper, thresholds = roc_curve(test_label.to_numpy(), test_pred)\n",
        "AUC = auc(fper, tper)\n",
        "plot_roc_curve(fper, tper, AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_GaMDGRVzgI"
      },
      "outputs": [],
      "source": [
        "fper, tper, thresholds = roc_curve(test_label.to_numpy(), probabilities[::,1])\n",
        "AUC = auc(fper, tper)\n",
        "plot_roc_curve(fper, tper, AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSo3E2PHc-Xz"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_preds, y_true, labels=None):\n",
        "  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
        "  fig, ax = plt.subplots(figsize=(6, 6))\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "  disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "  plt.title(\"Normalized confusion matrix\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soyXf9p9dDzC"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(test_pred, test_label.to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQgwH_WLVtzI"
      },
      "outputs": [],
      "source": [
        "d = {'pred', 'label', 'prob'}\n",
        "test_df = pd.DataFrame()\n",
        "\n",
        "test_df['pred'] = test_pred\n",
        "test_df['label'] = test_label\n",
        "test_df['prob'] = probabilities[::,1]\n",
        "\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWwzgyK53JxQ"
      },
      "source": [
        "## **Saving results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUe1AmYDCWQp"
      },
      "outputs": [],
      "source": [
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBC3bgHMi8py"
      },
      "outputs": [],
      "source": [
        "original_df = pd.read_csv(test_file_path)\n",
        "\n",
        "original_df['pred'] = test_pred\n",
        "original_df['prob'] = probabilities[::,1]\n",
        "\n",
        "os.makedirs(\"./BERTLarge\", exist_ok=True)\n",
        "output_file_path = './BERTLarge/test_predictions-BERTLarge.csv'\n",
        "original_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f'Arquivo com previsões e probabilidades salvo em: {output_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LfZaaQHAPMe"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import torch\n",
        "\n",
        "MODEL_PATH = \"/content/BERTLarge.bin\"\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "files.download(MODEL_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}