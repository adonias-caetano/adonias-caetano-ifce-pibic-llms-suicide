{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeCk7NLf_4z7"
   },
   "source": [
    "#  XAI alBERT - Suicide Detection\n",
    "**Autora:** Antonia Estefane Ribeiro Veras\n",
    "\n",
    "**Orientador:** Adonias Caetano de Oliveira\n",
    "\n",
    "**Instituição:** IFCE\n",
    "\n",
    "**Dataset disponível em:** https://www.kaggle.com/datasets/nikhileswarkomati/suicide-watch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-MIB0E2_9ll"
   },
   "source": [
    "## Instalação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3-CiHxmXE9_"
   },
   "outputs": [],
   "source": [
    "!pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJHEnkN7reNf"
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koCpDq1PX31j"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ksHDzoNX6no"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Kby9eDAEwx"
   },
   "source": [
    "## Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irNhcn15X9Bs"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp3iYSc_X_iC"
   },
   "outputs": [],
   "source": [
    "#text preprocessing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG3mNwHXYAke"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2twPdw73YEbS"
   },
   "outputs": [],
   "source": [
    "#text classification libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import seaborn as sns\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from scipy.special import expit\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxWV1IzDAVyl"
   },
   "source": [
    "## Carregamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSYswakWYJ5Z"
   },
   "outputs": [],
   "source": [
    "url = '(link púbico do Google Drive de acesso ao CSV de treinamento)'\n",
    "file_id = url.split('/')[-2]\n",
    "read_url='https://drive.google.com/uc?id=' + file_id\n",
    "\n",
    "dataset = pd.read_csv(read_url,  index_col=0)\n",
    "dataset = dataset[['selftext', 'is_suicide']]\n",
    "dataset = dataset.rename(columns={'selftext': 'text', 'is_suicide': 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udm3UqBhY1Y6"
   },
   "outputs": [],
   "source": [
    "data_process = dataset.copy()\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgdMYLXCAb9h"
   },
   "source": [
    "## Pré-Processamento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2HkQewYYz-k"
   },
   "outputs": [],
   "source": [
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lROd1XOaY7Q0"
   },
   "outputs": [],
   "source": [
    "old_texts = data_process[\"text\"]\n",
    "new_texts = []\n",
    "\n",
    "\n",
    "for text in old_texts:\n",
    "    if isinstance(text, str):  # Verifica se text é uma string\n",
    "        text = re.sub('@[^\\s]+', '', text)\n",
    "        text = unidecode(text)\n",
    "        text = re.sub('<[^<]+?>','', text)\n",
    "\n",
    "    else:\n",
    "        text = str(text)  # Converte para string\n",
    "        text = re.sub('@[^\\s]+', '', text)\n",
    "        text = unidecode(text)\n",
    "        text = re.sub('<[^<]+?>','', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JV_X238Y_js"
   },
   "outputs": [],
   "source": [
    "data_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E__XsOvKAifm"
   },
   "source": [
    "## Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcE6l_llZfoy"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x = data_process['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h55_fNWEAnA4"
   },
   "source": [
    "## Balanceamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxfjyMrjZlyi"
   },
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state= 0)\n",
    "X_bal, Y_bal = rus.fit_resample(data_process[['text']], data_process['class'])\n",
    "sns.countplot(x = Y_bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-RfiGz3Arhj"
   },
   "source": [
    "## Criação da Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFbL8tw3Z1fq"
   },
   "outputs": [],
   "source": [
    "new_texts = data_process[\"text\"]\n",
    "all_words = ' '.join([str(text) for text in new_texts if isinstance(text, str)])\n",
    "word_cloud = WordCloud(width= 800, height= 500, max_font_size = 110, background_color=\"white\", collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfQr3HADAxMv"
   },
   "source": [
    "## Divisão dos Dados em Conjuntos de Treinamento, Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac2L2D1ZZ-bz"
   },
   "outputs": [],
   "source": [
    "train_df, valid_df, train_label, valid_label = train_test_split(X_bal, Y_bal, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7TQLj7VA3tK"
   },
   "source": [
    "## Inicialização do Tokenizador e do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ti-RJsTbtBK"
   },
   "outputs": [],
   "source": [
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "N_labels = len(train_label.unique())\n",
    "\n",
    "# Inicialização do tokenizador ALBERT\n",
    "PRETRAINED_LM = 'albert-base-v2'\n",
    "tokenizer = AlbertTokenizer.from_pretrained(PRETRAINED_LM, do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PBZ-xLFA9xW"
   },
   "source": [
    "## Definição de Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0_7aAiBasNn"
   },
   "outputs": [],
   "source": [
    "def encode(docs):\n",
    "    '''\n",
    "    This function takes list of texts and returns input_ids and attention_mask of texts\n",
    "    '''\n",
    "    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                            return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_masks = encoded_dict['attention_mask']\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGdQlVJBBGOP"
   },
   "source": [
    "## Preparação dos dados para o treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEn5vxVQa34p"
   },
   "outputs": [],
   "source": [
    "train_input_ids, train_att_masks = encode(train_df['text'].values.tolist())\n",
    "valid_input_ids, valid_att_masks = encode(valid_df['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQ_95EXXcZFM"
   },
   "outputs": [],
   "source": [
    "train_y = torch.LongTensor(train_label.values)\n",
    "valid_y = torch.LongTensor(valid_label.values)\n",
    "train_y.size(),valid_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLVbBeOjccYI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdOHFZsvcfqP"
   },
   "outputs": [],
   "source": [
    "train_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkjZRX-ccih3"
   },
   "outputs": [],
   "source": [
    "model = AlbertForSequenceClassification.from_pretrained(PRETRAINED_LM, \n",
    "                                                        num_labels=N_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqDyhGFDclar"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EC7jWniPcmW1"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ubCJJBQcq2a"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 13\n",
    "LEARNING_RATE = 2e-6\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "             num_warmup_steps=0,\n",
    "            num_training_steps=len(train_dataloader)*EPOCHS )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B28sx2qgBNJI"
   },
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sk8ABQscwKO"
   },
   "outputs": [],
   "source": [
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader,desc='Training')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        del loss\n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss_per_epoch.append(train_loss / (step_num + 1))\n",
    "\n",
    "\n",
    "    '''\n",
    "    Validation\n",
    "    '''\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(valid_dataloader,desc='Validation')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "            loss = output.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "\n",
    "    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n",
    "    valid_pred = np.concatenate(valid_pred)\n",
    "\n",
    "    '''\n",
    "    Loss message\n",
    "    '''\n",
    "    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(train_df) / BATCH_SIZE), train_loss / (step_num + 1)))\n",
    "    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(valid_df) / BATCH_SIZE), valid_loss / (step_num_e + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPwKX12gBVPA"
   },
   "source": [
    "## Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cz7WO3Fgc0Qj"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "epochs = range(1, EPOCHS +1 )\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs,train_loss_per_epoch,label ='training loss')\n",
    "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
    "ax.set_title('Training and Validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O947EGtec5-X"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('classification report')\n",
    "print(classification_report(valid_pred, valid_label.to_numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSo3E2PHc-Xz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "def plot_confusion_matrix(y_preds, y_true, labels=None):\n",
    "  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "  fig, ax = plt.subplots(figsize=(6, 6))\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "  disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "  plt.title(\"Normalized confusion matrix\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soyXf9p9dDzC"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(valid_pred,valid_label.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zggB6wGtBfDc"
   },
   "source": [
    "## Interpretabilidade do Modelo com Lime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6G2y5crUeu_9"
   },
   "outputs": [],
   "source": [
    "def predict_proba(sentences):\n",
    "  model.eval()\n",
    "  probabilities = []\n",
    "\n",
    "  test_input_ids, test_att_masks = encode(sentences)\n",
    "  BATCH_SIZE = 16\n",
    "  test_y = torch.LongTensor([0] * len(sentences))\n",
    "  test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "  test_sampler = SequentialSampler(test_dataset)\n",
    "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "        probabilities.append(expit(output.logits.cpu().detach().numpy()))\n",
    "\n",
    "  probabilities = np.concatenate(probabilities)\n",
    "\n",
    "  return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlDZk8Xbodc0"
   },
   "outputs": [],
   "source": [
    "# Lista de frases para teste\n",
    "frases_teste = [\n",
    "    'feeling sick anxious need help please',\n",
    "    'doe anyone know specific name cleaning product mix kill oneself',\n",
    "    'assume death virus something else would le suicidal',\n",
    "    'want disappear painlessly',\n",
    "    'suffering humiliation worrying future bad im coward wanna die bad',\n",
    "    'always feel happy whole get one dont make even sad feel like something wrong',\n",
    "    'literally care anything else want happy',\n",
    "    'cheer celebrating another day packed activity sleeping lot wanting anything interested anything staring ceiling watching paint dry',\n",
    "    'win depression',\n",
    "    'today first appointment psychologist know going work might give ha worked'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkQ-pNC4uwSG"
   },
   "outputs": [],
   "source": [
    "labels_names = ['non-suicide', 'suicide']\n",
    "\n",
    "explainer = LimeTextExplainer(class_names = labels_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYAgzCtcoh5P"
   },
   "outputs": [],
   "source": [
    "# Loop sobre as frases de teste\n",
    "for frase in frases_teste:\n",
    "    # Gerando a explicação Lime para a frase atual\n",
    "    exp = explainer.explain_instance(frase, classifier_fn=predict_proba, num_features=10)\n",
    "    # Mostrando a explicação no console\n",
    "    print(\"Frase:\", frase)\n",
    "    exp.show_in_notebook(text=True)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1hYxSy0iWeEksTq-BQYxIa5p9b45dHJtJ",
     "timestamp": 1715044923742
    },
    {
     "file_id": "1gvyqJqJyJRMVCEolmDSVAQo7xw69z8iD",
     "timestamp": 1712059617036
    },
    {
     "file_id": "1kQpBvmEgivn1-TZmcsAhbhjQJwDAXJZ-",
     "timestamp": 1710824580573
    },
    {
     "file_id": "1nBdVcF6tZOyNNdOmSr3_HMFesAH6Alsq",
     "timestamp": 1710823520583
    },
    {
     "file_id": "1KCQmcXnqz0Vi9HhCVVL96JdN4GFO9XqK",
     "timestamp": 1710809505598
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
