{"cells":[{"cell_type":"markdown","source":["#  XAI alBERT - Suicide Detection\n","**Autora:** Antonia Estefane Ribeiro Veras\n","\n","**Orientador:** Adonias Caetano de Oliveira\n","\n","**Instituição:** IFCE\n","\n","**Dataset disponível em:**\n"],"metadata":{"id":"GeCk7NLf_4z7"}},{"cell_type":"markdown","source":["## Instalação de pacotes"],"metadata":{"id":"L-MIB0E2_9ll"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3-CiHxmXE9_"},"outputs":[],"source":["!pip install Unidecode"]},{"cell_type":"code","source":["!pip install lime"],"metadata":{"id":"AJHEnkN7reNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"koCpDq1PX31j"},"outputs":[],"source":["!pip install wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ksHDzoNX6no"},"outputs":[],"source":["#hide\n","!pip install transformers"]},{"cell_type":"markdown","source":["## Importação de Bibliotecas"],"metadata":{"id":"Z2Kby9eDAEwx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"irNhcn15X9Bs"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp3iYSc_X_iC"},"outputs":[],"source":["#text preprocessing libraries\n","import pandas as pd\n","import re\n","from unidecode import unidecode\n","from string import punctuation\n","import nltk\n","from nltk import sent_tokenize\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZG3mNwHXYAke"},"outputs":[],"source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2twPdw73YEbS"},"outputs":[],"source":["#text classification libraries\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import seaborn as sns\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.model_selection import train_test_split\n","from lime.lime_text import LimeTextExplainer\n","from scipy.special import expit\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFehe_h9XgRV"},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"markdown","source":["## Montagem do Drive"],"metadata":{"id":"UiuEXHukASSP"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"vMN2t5AcANK5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carregamento do Dataset"],"metadata":{"id":"AxWV1IzDAVyl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSYswakWYJ5Z"},"outputs":[],"source":["dataset = pd.read_csv('/content/drive/MyDrive/datasets/Suicide_Detection.csv')\n","# exibir as primeiras 10 linhas\n","dataset = dataset.rename(columns={'selftext_clean': 'text', 'is_suicide': 'class'})\n","\n","dataset.head(10)\n"]},{"cell_type":"markdown","source":["## Pré-Processamento de Texto"],"metadata":{"id":"pgdMYLXCAb9h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2HkQewYYz-k"},"outputs":[],"source":["nltk.download('rslp')\n","nltk.download('stopwords')\n","stopwords_list = stopwords.words(\"english\")\n","print(stopwords_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udm3UqBhY1Y6"},"outputs":[],"source":["data_process = dataset.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lROd1XOaY7Q0"},"outputs":[],"source":["import re\n","from unidecode import unidecode\n","\n","old_texts = data_process[\"text\"]\n","new_texts = []\n","\n","\n","for text in old_texts:\n","    if isinstance(text, str):  # Verifica se text é uma string\n","        text = re.sub('@[^\\s]+', '', text)\n","        text = unidecode(text)\n","        text = re.sub('<[^<]+?>','', text)\n","\n","    else:\n","        text = str(text)  # Converte para string\n","        text = re.sub('@[^\\s]+', '', text)\n","        text = unidecode(text)\n","        text = re.sub('<[^<]+?>','', text)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JV_X238Y_js"},"outputs":[],"source":["data_process"]},{"cell_type":"markdown","source":["## Visualização dos Dados"],"metadata":{"id":"E__XsOvKAifm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcE6l_llZfoy"},"outputs":[],"source":["sns.countplot(x = data_process['class'])"]},{"cell_type":"markdown","source":["### Balanceamento dos Dados"],"metadata":{"id":"h55_fNWEAnA4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxfjyMrjZlyi"},"outputs":[],"source":["rus = RandomUnderSampler(random_state= 0)\n","X_bal, Y_bal = rus.fit_resample(data_process[['text']], data_process['class'])\n","sns.countplot(x = Y_bal)"]},{"cell_type":"markdown","source":["### Criação da Wordcloud"],"metadata":{"id":"y-RfiGz3Arhj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFbL8tw3Z1fq"},"outputs":[],"source":["new_texts = data_process[\"text\"]\n","all_words = ' '.join([str(text) for text in new_texts if isinstance(text, str)])\n","word_cloud = WordCloud(width= 800, height= 500, max_font_size = 110, background_color=\"white\", collocations = False).generate(all_words)\n","plt.figure(figsize=(20,10))\n","plt.imshow(word_cloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","source":["## Divisão dos Dados em Conjuntos de Treinamento, Validação"],"metadata":{"id":"FfQr3HADAxMv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ac2L2D1ZZ-bz"},"outputs":[],"source":["# Dividir o dataset em conjunto de treinamento e validação\n","train_df, valid_df, train_label, valid_label = train_test_split(X_bal, Y_bal, test_size=0.20, random_state=42)\n"]},{"cell_type":"markdown","source":["## Inicialização do Tokenizador e do Modelo"],"metadata":{"id":"G7TQLj7VA3tK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ti-RJsTbtBK"},"outputs":[],"source":["from transformers import AlbertForSequenceClassification, AlbertTokenizer\n","N_labels = len(train_label.unique())\n","\n","# Inicialização do tokenizador ALBERT\n","PRETRAINED_LM = 'albert-base-v2'\n","tokenizer = AlbertTokenizer.from_pretrained(PRETRAINED_LM, do_lower_case=True)"]},{"cell_type":"markdown","source":["## Definição de Funções Auxiliares"],"metadata":{"id":"_PBZ-xLFA9xW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0_7aAiBasNn"},"outputs":[],"source":["def encode(docs):\n","    '''\n","    This function takes list of texts and returns input_ids and attention_mask of texts\n","    '''\n","    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=128, padding='max_length',\n","                            return_attention_mask=True, truncation=True, return_tensors='pt')\n","    input_ids = encoded_dict['input_ids']\n","    attention_masks = encoded_dict['attention_mask']\n","    return input_ids, attention_masks"]},{"cell_type":"markdown","source":["## Preparação dos dados para o treinamento do modelo"],"metadata":{"id":"qGdQlVJBBGOP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEn5vxVQa34p"},"outputs":[],"source":["train_input_ids, train_att_masks = encode(train_df['text'].values.tolist())\n","valid_input_ids, valid_att_masks = encode(valid_df['text'].values.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQ_95EXXcZFM"},"outputs":[],"source":["import torch\n","train_y = torch.LongTensor(train_label.values)\n","valid_y = torch.LongTensor(valid_label.values)\n","train_y.size(),valid_y.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLVbBeOjccYI"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","BATCH_SIZE = 16\n","train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n","valid_sampler = SequentialSampler(valid_dataset)\n","valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n","\n","test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdOHFZsvcfqP"},"outputs":[],"source":["train_label.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkjZRX-ccih3"},"outputs":[],"source":["# Inicialização do modelo ALBERT\n","model = AlbertForSequenceClassification.from_pretrained(PRETRAINED_LM, num_labels=N_labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqDyhGFDclar"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC7jWniPcmW1"},"outputs":[],"source":["model = model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ubCJJBQcq2a"},"outputs":[],"source":["from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","# Best results: 07 and 08\n","EPOCHS = 13\n","LEARNING_RATE = 2e-6\n","\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","             num_warmup_steps=0,\n","            num_training_steps=len(train_dataloader)*EPOCHS )"]},{"cell_type":"markdown","source":["## Treinamento do modelo"],"metadata":{"id":"B28sx2qgBNJI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sk8ABQscwKO"},"outputs":[],"source":["#collapse-output\n","from torch.nn.utils import clip_grad_norm_\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import math\n","\n","train_loss_per_epoch = []\n","val_loss_per_epoch = []\n","\n","\n","for epoch_num in range(EPOCHS):\n","    print('Epoch: ', epoch_num + 1)\n","    '''\n","    Training\n","    '''\n","    model.train()\n","    train_loss = 0\n","    for step_num, batch_data in enumerate(tqdm(train_dataloader,desc='Training')):\n","        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n","        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n","\n","        loss = output.loss\n","        train_loss += loss.item()\n","\n","        model.zero_grad()\n","        loss.backward()\n","        del loss\n","\n","        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    train_loss_per_epoch.append(train_loss / (step_num + 1))\n","\n","\n","    '''\n","    Validation\n","    '''\n","    model.eval()\n","    valid_loss = 0\n","    valid_pred = []\n","    with torch.no_grad():\n","        for step_num_e, batch_data in enumerate(tqdm(valid_dataloader,desc='Validation')):\n","            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n","            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n","\n","            loss = output.loss\n","            valid_loss += loss.item()\n","\n","            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n","\n","    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n","    valid_pred = np.concatenate(valid_pred)\n","\n","    '''\n","    Loss message\n","    '''\n","    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(train_df) / BATCH_SIZE), train_loss / (step_num + 1)))\n","    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(valid_df) / BATCH_SIZE), valid_loss / (step_num_e + 1)))"]},{"cell_type":"markdown","source":["## Avaliação do Modelo"],"metadata":{"id":"aPwKX12gBVPA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cz7WO3Fgc0Qj"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","epochs = range(1, EPOCHS +1 )\n","fig, ax = plt.subplots()\n","ax.plot(epochs,train_loss_per_epoch,label ='training loss')\n","ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n","ax.set_title('Training and Validation loss')\n","ax.set_xlabel('Epochs')\n","ax.set_ylabel('Loss')\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O947EGtec5-X"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","print('classifiation report')\n","print(classification_report(valid_pred, valid_label.to_numpy()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSo3E2PHc-Xz"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","def plot_confusion_matrix(y_preds, y_true, labels=None):\n","  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n","  fig, ax = plt.subplots(figsize=(6, 6))\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","  disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n","  plt.title(\"Normalized confusion matrix\")\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soyXf9p9dDzC"},"outputs":[],"source":["plot_confusion_matrix(valid_pred,valid_label.to_numpy())"]},{"cell_type":"markdown","source":["## Interpretabilidade do Modelo com Lime\n"],"metadata":{"id":"zggB6wGtBfDc"}},{"cell_type":"code","source":["def predict_proba(sentences):\n","  model.eval()\n","  probabilities = []\n","\n","  test_input_ids, test_att_masks = encode(sentences)\n","  BATCH_SIZE = 16\n","  test_y = torch.LongTensor([0] * len(sentences))\n","  test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n","  test_sampler = SequentialSampler(test_dataset)\n","  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","  with torch.no_grad():\n","    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n","        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n","        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n","\n","        probabilities.append(expit(output.logits.cpu().detach().numpy()))\n","\n","  probabilities = np.concatenate(probabilities)\n","\n","  return probabilities"],"metadata":{"id":"6G2y5crUeu_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lista de frases para teste\n","frases_teste = [\n","    'feeling sick anxious need help please',\n","    'doe anyone know specific name cleaning product mix kill oneself',\n","    'assume death virus something else would le suicidal',\n","    'want disappear painlessly',\n","    'suffering humiliation worrying future bad im coward wanna die bad',\n","    'always feel happy whole get one dont make even sad feel like something wrong',\n","    'literally care anything else want happy',\n","    'cheer celebrating another day packed activity sleeping lot wanting anything interested anything staring ceiling watching paint dry',\n","    'win depression',\n","    'today first appointment psychologist know going work might give ha worked'\n","]"],"metadata":{"id":"rlDZk8Xbodc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_names = ['non-suicide', 'suicide']\n","\n","explainer = LimeTextExplainer(class_names = labels_names)"],"metadata":{"id":"xkQ-pNC4uwSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loop sobre as frases de teste\n","for frase in frases_teste:\n","    # Gerando a explicação Lime para a frase atual\n","    exp = explainer.explain_instance(frase, classifier_fn=predict_proba, num_features=10)\n","    # Mostrando a explicação no console\n","    print(\"Frase:\", frase)\n","    exp.show_in_notebook(text=True)\n","    print(\"\\n\")"],"metadata":{"id":"EYAgzCtcoh5P"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1hYxSy0iWeEksTq-BQYxIa5p9b45dHJtJ","timestamp":1715044923742},{"file_id":"1gvyqJqJyJRMVCEolmDSVAQo7xw69z8iD","timestamp":1712059617036},{"file_id":"1kQpBvmEgivn1-TZmcsAhbhjQJwDAXJZ-","timestamp":1710824580573},{"file_id":"1nBdVcF6tZOyNNdOmSr3_HMFesAH6Alsq","timestamp":1710823520583},{"file_id":"1KCQmcXnqz0Vi9HhCVVL96JdN4GFO9XqK","timestamp":1710809505598}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}