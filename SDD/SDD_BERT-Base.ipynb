{"cells":[{"cell_type":"markdown","source":["#  XAI BERT-\n","\n","\n","**Autora:** Antonia Estefane Ribeiro Veras\n","\n","**Orientador:** Adonias Caetano de Oliveira\n","\n","**Instituição:** IFCE\n","\n","**Dataset disponível em:**\n"],"metadata":{"id":"uaqJ7eVlBv09"}},{"cell_type":"markdown","source":["## Instalação de Pacotes"],"metadata":{"id":"4c3ex9p6B6a2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3-CiHxmXE9_"},"outputs":[],"source":["!pip install Unidecode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"koCpDq1PX31j"},"outputs":[],"source":["!pip install wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ksHDzoNX6no"},"outputs":[],"source":["#hide\n","!pip install transformers"]},{"cell_type":"code","source":["!pip install lime"],"metadata":{"id":"qIpA3pJtMVBw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Importação de Bibliotecas"],"metadata":{"id":"jhGy9D_7B9Z9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"irNhcn15X9Bs"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","source":["from lime.lime_text import LimeTextExplainer"],"metadata":{"id":"YyAAWiXpMfZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp3iYSc_X_iC"},"outputs":[],"source":["#text preprocessing libraries\n","import pandas as pd\n","import re\n","from unidecode import unidecode\n","from string import punctuation\n","import nltk\n","from nltk import sent_tokenize\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZG3mNwHXYAke"},"outputs":[],"source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2twPdw73YEbS"},"outputs":[],"source":["#text classification libraries\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import seaborn as sns\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.model_selection import train_test_split\n","from scipy.special import expit\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFehe_h9XgRV"},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"markdown","source":["## Montagem do Drive"],"metadata":{"id":"zCqZy8NyCFkf"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"xj8nh7AjCC11"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carregamento do Dataset"],"metadata":{"id":"tppRsT1ZCHbZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSYswakWYJ5Z"},"outputs":[],"source":["\n","dataset = pd.read_csv('/content/drive/MyDrive/datasets/combined.csv')\n","#dataset = dataset[['selftext_clean', 'is_suicide']]\n","dataset = dataset.rename(columns={'selftext_clean': 'text', 'is_suicide': 'class'})\n","\n","dataset.head(10)\n"]},{"cell_type":"markdown","source":["## Pré-Processamento"],"metadata":{"id":"o6JXGotPll_V"}},{"cell_type":"code","source":["nltk.download('rslp')\n","nltk.download('stopwords')\n","stopwords_list = stopwords.words(\"english\")\n","print(stopwords_list)"],"metadata":{"id":"9vlBAZdjMTPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udm3UqBhY1Y6"},"outputs":[],"source":["data_process = dataset.copy()"]},{"cell_type":"code","source":["import re\n","from unidecode import unidecode\n","\n","old_texts = data_process[\"text\"]\n","new_texts = []\n","\n","\n","for text in old_texts:\n","    if isinstance(text, str):  # Verifica se text é uma string\n","        text = re.sub('@[^\\s]+', '', text)\n","        text = unidecode(text)\n","        text = re.sub('<[^<]+?>','', text)\n","\n","    else:\n","        text = str(text)  # Converte para string\n","        text = re.sub('@[^\\s]+', '', text)\n","        text = unidecode(text)\n","        text = re.sub('<[^<]+?>','', text)\n","\n"],"metadata":{"id":"60DywcxMTatr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_process"],"metadata":{"id":"byIh9i0UTelA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualização dos Dados"],"metadata":{"id":"Ee1MXhXjCRyf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcE6l_llZfoy"},"outputs":[],"source":["sns.countplot(x = data_process['class'])"]},{"cell_type":"markdown","source":["### Balanceamento dos Dados\n","\n"],"metadata":{"id":"LNlCaOVsCU7b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxfjyMrjZlyi"},"outputs":[],"source":["rus = RandomUnderSampler(random_state= 0)\n","X_bal, Y_bal = rus.fit_resample(data_process[['text']], data_process['class'])\n","sns.countplot(x = Y_bal)"]},{"cell_type":"markdown","source":["### Criação da Wordcloud"],"metadata":{"id":"fJ6EmJ1ICX0A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFbL8tw3Z1fq"},"outputs":[],"source":["new_texts = data_process[\"text\"]\n","all_words = ' '.join([str(text) for text in new_texts if isinstance(text, str)])\n","word_cloud = WordCloud(width= 800, height= 500, max_font_size = 110, background_color=\"white\", collocations = False).generate(all_words)\n","plt.figure(figsize=(20,10))\n","plt.imshow(word_cloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","source":["## Divisão dos Dados em Conjuntos de Treinamento, Validação"],"metadata":{"id":"uUhcY-XYCcbp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ac2L2D1ZZ-bz"},"outputs":[],"source":["# Dividir o dataset em conjunto de treinamento e validação\n","train_df, valid_df, train_label, valid_label = train_test_split(X_bal, Y_bal, test_size=0.20, random_state=42)\n"]},{"cell_type":"markdown","source":["## Inicialização do Tokenizador e do Modelo"],"metadata":{"id":"OeP2uHqdCjHf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ti-RJsTbtBK"},"outputs":[],"source":["PRETRAINED_LM = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM, do_lower_case=True)\n","tokenizer"]},{"cell_type":"markdown","source":["## Definição de Funções Auxiliares"],"metadata":{"id":"BEdlzECHCnw4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0_7aAiBasNn"},"outputs":[],"source":["def encode(docs):\n","    '''\n","    This function takes list of texts and returns input_ids and attention_mask of texts\n","    '''\n","    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=128, padding='max_length',\n","                            return_attention_mask=True, truncation=True, return_tensors='pt')\n","    input_ids = encoded_dict['input_ids']\n","    attention_masks = encoded_dict['attention_mask']\n","    return input_ids, attention_masks"]},{"cell_type":"markdown","source":["## Preparação dos dados para o treinamento do modelo"],"metadata":{"id":"ch3SSX5WCsBw"}},{"cell_type":"code","source":["train_input_ids, train_att_masks = encode(train_df['text'].values.tolist())\n","valid_input_ids, valid_att_masks = encode(valid_df['text'].values.tolist())"],"metadata":{"id":"3KMLXBCtRbic"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQ_95EXXcZFM"},"outputs":[],"source":["import torch\n","train_y = torch.LongTensor(train_label.values)\n","valid_y = torch.LongTensor(valid_label.values)\n","train_y.size(),valid_y.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLVbBeOjccYI"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","BATCH_SIZE = 16\n","train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n","valid_sampler = SequentialSampler(valid_dataset)\n","valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdOHFZsvcfqP"},"outputs":[],"source":["train_label.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkjZRX-ccih3"},"outputs":[],"source":["from transformers import BertForSequenceClassification\n","N_labels = len(train_label.unique())\n","model = BertForSequenceClassification.from_pretrained(PRETRAINED_LM,\n","                                                      num_labels=N_labels,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqDyhGFDclar"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC7jWniPcmW1"},"outputs":[],"source":["model = model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ubCJJBQcq2a"},"outputs":[],"source":["from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","# Best results: 07 and 08\n","EPOCHS = 4\n","LEARNING_RATE = 2e-6\n","\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","             num_warmup_steps=0,\n","            num_training_steps=len(train_dataloader)*EPOCHS )"]},{"cell_type":"markdown","source":["## Treinamento do modelo"],"metadata":{"id":"m94lfbg-C4f0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sk8ABQscwKO"},"outputs":[],"source":["#collapse-output\n","from torch.nn.utils import clip_grad_norm_\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import math\n","\n","train_loss_per_epoch = []\n","val_loss_per_epoch = []\n","\n","\n","for epoch_num in range(EPOCHS):\n","    print('Epoch: ', epoch_num + 1)\n","    '''\n","    Training\n","    '''\n","    model.train()\n","    train_loss = 0\n","    for step_num, batch_data in enumerate(tqdm(train_dataloader,desc='Training')):\n","        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n","        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n","\n","        loss = output.loss\n","        train_loss += loss.item()\n","\n","        model.zero_grad()\n","        loss.backward()\n","        del loss\n","\n","        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    train_loss_per_epoch.append(train_loss / (step_num + 1))\n","\n","\n","    '''\n","    Validation\n","    '''\n","    model.eval()\n","    valid_loss = 0\n","    valid_pred = []\n","    with torch.no_grad():\n","        for step_num_e, batch_data in enumerate(tqdm(valid_dataloader,desc='Validation')):\n","            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n","            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n","\n","            loss = output.loss\n","            valid_loss += loss.item()\n","\n","            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n","\n","    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n","    valid_pred = np.concatenate(valid_pred)\n","\n","    '''\n","    Loss message\n","    '''\n","    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(train_df) / BATCH_SIZE), train_loss / (step_num + 1)))\n","    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(valid_df) / BATCH_SIZE), valid_loss / (step_num_e + 1)))"]},{"cell_type":"markdown","source":["## Avaliação do Modelo"],"metadata":{"id":"R1vHwgXPC-Fi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cz7WO3Fgc0Qj"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","epochs = range(1, EPOCHS +1 )\n","fig, ax = plt.subplots()\n","ax.plot(epochs,train_loss_per_epoch,label ='training loss')\n","ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n","ax.set_title('Training and Validation loss')\n","ax.set_xlabel('Epochs')\n","ax.set_ylabel('Loss')\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O947EGtec5-X"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","print('classifiation report')\n","print(classification_report(valid_pred, valid_label.to_numpy()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSo3E2PHc-Xz"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","def plot_confusion_matrix(y_preds, y_true, labels=None):\n","  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n","  fig, ax = plt.subplots(figsize=(6, 6))\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","  disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n","  plt.title(\"Normalized confusion matrix\")\n","  plt.show()"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def addlabels(x, y):\n","    for i in range(len(x)):\n","        plt.text(y[i], i, str(f'{y[i]*100:0.2f}%'), ha='center', bbox=dict(facecolor='blue', alpha=.6))\n","\n","plt.clf()\n","\n","pos = np.arange(len(df['Label'].values))\n","probabilidades = df['Probabilidade'].values\n","\n","# Definir as cores para cada classe\n","colors = df['Label'].map({'non-suicide': 'g', 'suicide': 'r'})\n","\n","# Criar o gráfico de barras\n","fig = plt.barh(pos, probabilidades, color=colors, edgecolor='black')\n","plt.yticks(pos, df['Label'])  # Exibir cada classe no eixo y\n","plt.legend(fig, [str(i) for i in ['Negativo', 'Positivo']])  # Exibir a legenda de cada classe (Positivo ou Negativo)\n","plt.xlabel('Probabilidades', fontsize=16)\n","plt.ylabel('Classe', fontsize=16)\n","addlabels(pos, probabilidades)  # Chamar a função para adicionar rótulos de valores\n","plt.title('Probabilidades de Predição', fontsize=16)\n","plt.show()\n"],"metadata":{"id":"BZgQewuvH9JZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soyXf9p9dDzC"},"outputs":[],"source":["plot_confusion_matrix(valid_pred,valid_label.to_numpy())"]},{"cell_type":"code","source":["# Lista de frases para teste\n","frases_teste = [\n","    'feeling sick anxious need help please',\n","    'doe anyone know specific name cleaning product mix kill oneself',\n","    'assume death virus something else would le suicidal',\n","    'want disappear painlessly',\n","    'suffering humiliation worrying future bad im coward wanna die bad',\n","    'always feel happy whole get one dont make even sad feel like something wrong',\n","    'literally care anything else want happy',\n","    'cheer celebrating another day packed activity sleeping lot wanting anything interested anything staring ceiling watching paint dry',\n","    'win depression',\n","    'today first appointment psychologist know going work might give ha worked'\n","]"],"metadata":{"id":"WdgJHF4BPPHm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Interpretabilidade do Modelo com Lime"],"metadata":{"id":"skg5sMRvDELW"}},{"cell_type":"code","source":["def createDataloader(text):\n","  test_input_ids, test_att_masks = encode([text])\n","  BATCH_SIZE = 16\n","  test_y = torch.LongTensor([0])\n","  test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n","  test_sampler = SequentialSampler(test_dataset)\n","  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n","  return test_dataloader"],"metadata":{"id":"QsqEgkjQH0LW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_proba(sentences):\n","  model.eval()\n","  probabilities = []\n","\n","  test_input_ids, test_att_masks = encode(sentences)\n","  BATCH_SIZE = 16\n","  test_y = torch.LongTensor([0] * len(sentences))\n","  test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n","  test_sampler = SequentialSampler(test_dataset)\n","  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","  with torch.no_grad():\n","    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n","        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n","        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n","\n","        probabilities.append(expit(output.logits.cpu().detach().numpy()))\n","\n","  probabilities = np.concatenate(probabilities)\n","\n","  return probabilities"],"metadata":{"id":"6G2y5crUeu_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_names = ['non-suicide', 'suicide']\n","# Loop sobre as frases de teste\n","for frase in frases_teste:\n","    # Gerando a explicação Lime para a frase atual\n","    exp = explainer.explain_instance(frase, classifier_fn=predict_proba, num_features=10)\n","    # Mostrando a explicação no console\n","    print(\"Frase:\", frase)\n","    exp.show_in_notebook(text=True)\n","    print(\"\\n\")"],"metadata":{"id":"fenFyT8PhscP"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1nBdVcF6tZOyNNdOmSr3_HMFesAH6Alsq","timestamp":1714436257888},{"file_id":"1KCQmcXnqz0Vi9HhCVVL96JdN4GFO9XqK","timestamp":1710809505598}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}