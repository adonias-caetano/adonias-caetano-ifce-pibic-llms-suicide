{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaqJ7eVlBv09"
   },
   "source": [
    "#  Mental-RoBERTa\n",
    "**Autora:** Antonia Estefane Ribeiro Veras\n",
    "\n",
    "**Orientador:** Adonias Caetano de Oliveira\n",
    "\n",
    "**Instituição:** IFCE\n",
    "\n",
    "**Dataset disponível em:** https://www.kaggle.com/datasets/nikhileswarkomati/suicide-watch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c3ex9p6B6a2"
   },
   "source": [
    "## Instalação de Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3-CiHxmXE9_"
   },
   "outputs": [],
   "source": [
    "!pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koCpDq1PX31j"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ksHDzoNX6no"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIpA3pJtMVBw"
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhGy9D_7B9Z9"
   },
   "source": [
    "## Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irNhcn15X9Bs"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp3iYSc_X_iC"
   },
   "outputs": [],
   "source": [
    "#text preprocessing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG3mNwHXYAke"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2twPdw73YEbS"
   },
   "outputs": [],
   "source": [
    "#text classification libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import seaborn as sns\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import expit\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyAAWiXpMfZJ"
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tppRsT1ZCHbZ"
   },
   "source": [
    "## Carregamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLGOXwT-Phss"
   },
   "outputs": [],
   "source": [
    "url = '(link púbico do Google Drive de acesso ao CSV de treinamento)'\n",
    "file_id = url.split('/')[-2]\n",
    "read_url='https://drive.google.com/uc?id=' + file_id\n",
    "\n",
    "dataset = pd.read_csv(read_url,  index_col=0)\n",
    "dataset = dataset[['selftext', 'is_suicide']]\n",
    "dataset = dataset.rename(columns={'selftext': 'text', 'is_suicide': 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process = dataset.copy()\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nzE82yCCKdV"
   },
   "source": [
    "## Pré-Processamento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2HkQewYYz-k"
   },
   "outputs": [],
   "source": [
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lROd1XOaY7Q0"
   },
   "outputs": [],
   "source": [
    "# Obtendo os textos originais\n",
    "old_texts = data_process[\"text\"]\n",
    "new_texts = []\n",
    "\n",
    "for text in old_texts:\n",
    "    # Verifique se o texto não é um valor nulo (NaN)\n",
    "    if isinstance(text, str):\n",
    "        # Converte para minúsculas\n",
    "        text = text.lower()\n",
    "        # Remove menções (padrão do Twitter)\n",
    "        text = re.sub('@[^\\s]+', '', text)\n",
    "        # Remove caracteres especiais do HTML\n",
    "        text = re.sub('<[^<]+?>','', text)\n",
    "        # Remove dígitos\n",
    "        text = ''.join(c for c in text if not c.isdigit())\n",
    "        # Remove URLs\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
    "        # Remove pontuação\n",
    "        text = ''.join(c for c in text if c not in punctuation)\n",
    "        # Remove stopwords\n",
    "        text = ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "        # Remove aspas duplas e simples\n",
    "        text = text.replace(\"\\\"\", \"\").replace(\"'\", \"\")\n",
    "        # Remove caracteres acentuados\n",
    "        text = unidecode(text)\n",
    "        # Adiciona o texto pré-processado à lista\n",
    "        new_texts.append(text)\n",
    "    else:\n",
    "        # Se o texto for um valor nulo, você pode optar por ignorá-lo ou realizar outra ação adequada\n",
    "        new_texts.append(\"\")  # Aqui estou adicionando uma string vazia como um texto substituto\n",
    "\n",
    "# Atualiza a coluna \"text\" com os textos pré-processados\n",
    "data_process[\"text\"] = new_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JV_X238Y_js"
   },
   "outputs": [],
   "source": [
    "data_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee1MXhXjCRyf"
   },
   "source": [
    "## Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcE6l_llZfoy"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x = data_process['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNlCaOVsCU7b"
   },
   "source": [
    "## Balanceamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxfjyMrjZlyi"
   },
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state= 0)\n",
    "X_bal, Y_bal = rus.fit_resample(data_process[['text']], data_process['class'])\n",
    "sns.countplot(x = Y_bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ6EmJ1ICX0A"
   },
   "source": [
    "## Criação da Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFbL8tw3Z1fq"
   },
   "outputs": [],
   "source": [
    "new_texts = data_process[\"text\"]\n",
    "all_words = ' '.join([text for text in new_texts])\n",
    "word_cloud = WordCloud(width= 800, height= 500, max_font_size = 110, background_color=\"white\", collocations = False).generate(all_words)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUhcY-XYCcbp"
   },
   "source": [
    "## Divisão dos Dados em Conjuntos de Treinamento, Validação e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac2L2D1ZZ-bz"
   },
   "outputs": [],
   "source": [
    "train_df, test_df, train_label, test_label = train_test_split(X_bal, Y_bal, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9uPB70FaDUQ"
   },
   "outputs": [],
   "source": [
    "train_df, valid_df, train_label,  valid_label = train_test_split(train_df, train_label, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN4Wd_eNPaCk"
   },
   "outputs": [],
   "source": [
    "label_names = ['non-suicide', 'suicide']\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeP2uHqdCjHf"
   },
   "source": [
    "## Inicialização do Tokenizador e do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ti-RJsTbtBK"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "N_labels = len(train_label.unique())\n",
    "\n",
    "PRETRAINED_LM = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM, do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEdlzECHCnw4"
   },
   "source": [
    "## Definição de Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0_7aAiBasNn"
   },
   "outputs": [],
   "source": [
    "def encode(docs):\n",
    "    '''\n",
    "    This function takes list of texts and returns input_ids and attention_mask of texts\n",
    "    '''\n",
    "    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                            return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_masks = encoded_dict['attention_mask']\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch3SSX5WCsBw"
   },
   "source": [
    "## Preparação dos dados para o treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEn5vxVQa34p"
   },
   "outputs": [],
   "source": [
    "train_input_ids, train_att_masks = encode(train_df['text'].values.tolist())\n",
    "valid_input_ids, valid_att_masks = encode(valid_df['text'].values.tolist())\n",
    "test_input_ids, test_att_masks = encode(test_df['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQ_95EXXcZFM"
   },
   "outputs": [],
   "source": [
    "train_y = torch.LongTensor(train_label.values)\n",
    "valid_y = torch.LongTensor(valid_label.values)\n",
    "test_y = torch.LongTensor(test_label.values)\n",
    "train_y.size(),valid_y.size(),test_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLVbBeOjccYI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = TensorDataset(train_input_ids, train_att_masks, train_y)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, valid_y)\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdOHFZsvcfqP"
   },
   "outputs": [],
   "source": [
    "train_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkjZRX-ccih3"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_LM,\n",
    "                                                      num_labels=N_labels,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqDyhGFDclar"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EC7jWniPcmW1"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ubCJJBQcq2a"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 7\n",
    "LEARNING_RATE = 2e-6\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "             num_warmup_steps=0,\n",
    "            num_training_steps=len(train_dataloader)*EPOCHS )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m94lfbg-C4f0"
   },
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sk8ABQscwKO"
   },
   "outputs": [],
   "source": [
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader,desc='Training')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        del loss\n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss_per_epoch.append(train_loss / (step_num + 1))\n",
    "\n",
    "\n",
    "    '''\n",
    "    Validation\n",
    "    '''\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(valid_dataloader,desc='Validation')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "            loss = output.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            valid_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "\n",
    "    val_loss_per_epoch.append(valid_loss / (step_num_e + 1))\n",
    "    valid_pred = np.concatenate(valid_pred)\n",
    "\n",
    "    '''\n",
    "    Loss message\n",
    "    '''\n",
    "    print(\"{0}/{1} train loss: {2} \".format(step_num+1, math.ceil(len(train_df) / BATCH_SIZE), train_loss / (step_num + 1)))\n",
    "    print(\"{0}/{1} val loss: {2} \".format(step_num_e+1, math.ceil(len(valid_df) / BATCH_SIZE), valid_loss / (step_num_e + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cz7WO3Fgc0Qj"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "epochs = range(1, EPOCHS +1 )\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs,train_loss_per_epoch,label ='training loss')\n",
    "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
    "ax.set_title('Training and Validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1I8IBJUUcKs"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_pred = []\n",
    "test_loss= 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        test_pred.append(np.argmax(output.logits.cpu().detach().numpy(),axis=-1))\n",
    "test_pred = np.concatenate(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1vHwgXPC-Fi"
   },
   "source": [
    "## Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O947EGtec5-X"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('classification report')\n",
    "print(classification_report(test_pred, test_label.to_numpy(),target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hk_D_bFsDh-f"
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "model.eval()\n",
    "probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
    "      input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "      output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "      prob = expit(output.logits.cpu().detach().numpy())\n",
    "\n",
    "      probabilities.append( prob )\n",
    "\n",
    "probabilities = np.concatenate(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1poHrRfDjX2"
   },
   "outputs": [],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPjKFuMufRSX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(fper, tper, AUC):\n",
    "    plt.plot(fper, tper, color=\"red\", label=f\"AUC = {AUC}\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"green\", linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "fper, tper, thresholds = roc_curve(test_label.to_numpy(), test_pred.to_numpy())\n",
    "AUC = auc(fper, tper)\n",
    "plot_roc_curve(fper, tper, AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_GaMDGRVzgI"
   },
   "outputs": [],
   "source": [
    "fper, tper, thresholds = roc_curve(test_label.to_numpy(), probabilities[::,1])\n",
    "AUC = auc(fper, tper)\n",
    "plot_roc_curve(fper, tper, AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSo3E2PHc-Xz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "def plot_confusion_matrix(y_preds, y_true, labels=None):\n",
    "  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "  fig, ax = plt.subplots(figsize=(6, 6))\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "  disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "  plt.title(\"Normalized confusion matrix\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soyXf9p9dDzC"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_pred, test_label.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQgwH_WLVtzI"
   },
   "outputs": [],
   "source": [
    "test_df['pred'] = test_pred\n",
    "test_df['label'] = test_label\n",
    "test_df['prob'] = probabilities\n",
    "test_df.reset_index(level=0)\n",
    "print(test_df[test_df['label']!=test_df['pred']].shape)\n",
    "test_df[test_df['label']!=test_df['pred']][['text','label','pred']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWwzgyK53JxQ"
   },
   "source": [
    "## **Saving results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUe1AmYDCWQp"
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYEsqMZo2bcw"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "test_df.to_csv(\"test_results_BERTimbauLarge.csv\",index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGj0gb6bCrp9"
   },
   "outputs": [],
   "source": [
    "shutil.copy('/content/test_results_BERTimbauLarge.csv', '/content/drive/MyDrive/test_results_BERTimbauLarge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LfZaaQHAPMe"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"BERTimbauLarge.bin\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4pTVeA1S89D"
   },
   "outputs": [],
   "source": [
    "shutil.copy('/content/BERTimbauLarge.bin', '/content/drive/MyDrive/BERTimbauLarge.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skg5sMRvDELW"
   },
   "source": [
    "## Interpretabilidade do Modelo com Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsqEgkjQH0LW"
   },
   "outputs": [],
   "source": [
    "def createDataloader(text):\n",
    "  test_input_ids, test_att_masks = encode([text])\n",
    "  BATCH_SIZE = 16\n",
    "  test_y = torch.LongTensor([0])\n",
    "  test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "  test_sampler = SequentialSampler(test_dataset)\n",
    "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "  return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtZOtHb-H2aj"
   },
   "outputs": [],
   "source": [
    "def predict(text, labels_names):\n",
    "    model.eval()\n",
    "    test_dataloader = createDataloader(text)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            output = model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "            logits = output.logits.cpu().detach().numpy()\n",
    "            index_pred = np.argmax(logits, axis=-1)[0]\n",
    "            probabilities = expit(logits)[0]\n",
    "\n",
    "    df = pd.DataFrame(columns=['Label', 'Probabilidade'])\n",
    "    df['Label'] = labels_names\n",
    "    df['Probabilidade'] = probabilities\n",
    "\n",
    "    return index_pred, labels_names[index_pred], df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3h9sVuEH7Od"
   },
   "outputs": [],
   "source": [
    "labels_names = ['non-suicide', 'suicide']\n",
    "text = 'think better night posts pretty shit shitposts rn like 7 ish hours making shitty posts sorry'\n",
    "index_pred, label, df = predict(text, labels_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZgQewuvH9JZ"
   },
   "outputs": [],
   "source": [
    "def addlabels(x, y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(y[i], i, str(f'{y[i]*100:0.2f}%'), ha='center', bbox=dict(facecolor='blue', alpha=.6))\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "pos = np.arange(len(df['Label'].values))\n",
    "probabilidades = df['Probabilidade'].values\n",
    "\n",
    "# Definir as cores para cada classe\n",
    "colors = df['Label'].map({'non-suicide': 'g', 'suicide': 'r'})\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "fig = plt.barh(pos, probabilidades, color=colors, edgecolor='black')\n",
    "plt.yticks(pos, df['Label'])  # Exibir cada classe no eixo y\n",
    "plt.legend(fig, [str(i) for i in ['Negativo', 'Positivo']])  # Exibir a legenda de cada classe (Positivo ou Negativo)\n",
    "plt.xlabel('Probabilidades', fontsize=16)\n",
    "plt.ylabel('Classe', fontsize=16)\n",
    "addlabels(pos, probabilidades)  # Chamar a função para adicionar rótulos de valores\n",
    "plt.title('Probabilidades de Predição', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6G2y5crUeu_9"
   },
   "outputs": [],
   "source": [
    "def predict_proba(sentences):\n",
    "  model.eval()\n",
    "  probabilities = []\n",
    "\n",
    "  test_input_ids, test_att_masks = encode(sentences)\n",
    "  BATCH_SIZE = 16\n",
    "  test_y = torch.LongTensor([0] * len(sentences))\n",
    "  test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "  test_sampler = SequentialSampler(test_dataset)\n",
    "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "\n",
    "        probabilities.append(expit(output.logits.cpu().detach().numpy()))\n",
    "\n",
    "  probabilities = np.concatenate(probabilities)\n",
    "\n",
    "  return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe-66SBlRr63"
   },
   "outputs": [],
   "source": [
    "labels_names = ['non-suicide', 'suicide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ-MAf5NH3iQ"
   },
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names = labels_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaplPhv2IHIk"
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance('know 7 months self harm free urge gets stronger stronger every day past days shit family stresses especially nephew 12 mum full custody told go kill numerous times wish easy partners family stresses especially past days stress trigger want scream cry live near woods could go scream till ca possibly scream anymore cops probably get called oh knows depression really know feeling moment would want come make sure ok fucking annoying annoys time small stupid things even know going post', classifier_fn = predict_proba, num_features = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "797Ms99uJr-H"
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1S8KhbVQxh1"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(exp.as_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKBqjdOfIlMr"
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance('feeling depressed stressed someone actually caring talk feel like dearth caring affectionate people planet everyone terrible indifferent write paragraphs paragraphs text make someone feel important never get return', classifier_fn = predict_proba, num_features = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlrjLIf7In1D"
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7YaqDlQIq0K"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(exp.as_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlgUxfzUIs6J"
   },
   "outputs": [],
   "source": [
    "def execute_classify(input):\n",
    "\n",
    "  if model != None:\n",
    "    test_pred = predict(input)\n",
    "    label = labels_names[test_pred[0]]\n",
    "    return f\"A classe prevista foi: {label}\"\n",
    "  else:\n",
    "    return \"Não há texto ou modelo para classificação!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsZXuYhvIuj3"
   },
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y53vZCmbw36-"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
